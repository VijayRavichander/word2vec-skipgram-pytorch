{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Skip Gram Model using Pytorch\n",
    "\n",
    "* Checkout this repo for more details: https://github.com/OlgaChernytska/word2vec-pytorch\n",
    "* Well explained article: https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SKIPGRAM_N_WORDS = 4\n",
    "MAX_SEQUENCE_LENGTH = 258\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIMENSION = 128\n",
    "EMBED_NORM = 1\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def get_eng_tokenizer():\n",
    "    tokenizer = get_tokenizer('basic_english', language = \"en\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the data\n",
    "#data_iter = WikiText2(root='/Users/vijayravichander/Code/CS224N/data', split = 'train')\n",
    "data_iter = WikiText2(root='data', split = 'train')\n",
    "data_iter = to_map_style_dataset(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_eng_tokenizer()\n",
    "# To create the vocab from the dataset \n",
    "vocab = build_vocab_from_iterator(map(tokenizer, data_iter), specials = [\"<unk>\"], min_freq= 50)\n",
    "# All news words will be have \"unk\" token\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'quite', 'good']\n",
      "[1016, 2849, 23, 2189, 423]\n"
     ]
    }
   ],
   "source": [
    "#Example of tokenizer\n",
    "example_text = \"Machine Learning is quite good\"\n",
    "print(tokenizer(example_text))\n",
    "print(vocab(tokenizer(example_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mapping processed words into\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data:  \n",
      "\n",
      " Length: 2\n",
      " Tokenized: []\n",
      "Formatted Text: []\n",
      "Length of tokens: 0\n",
      "-----------------\n",
      "Current data:  = Valkyria Chronicles III = \n",
      "\n",
      " Length: 30\n",
      " Tokenized: ['=', 'valkyria', 'chronicles', 'iii', '=']\n",
      "Formatted Text: [9, 3849, 3869, 881, 9]\n",
      "Length of tokens: 5\n",
      "-----------------\n",
      "Current data:  \n",
      "\n",
      " Length: 2\n",
      " Tokenized: []\n",
      "Formatted Text: []\n",
      "Length of tokens: 0\n",
      "-----------------\n",
      "Current data:  Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n",
      " Length: 694\n",
      " Tokenized: ['senjō', 'no', 'valkyria', '3', '<unk>', 'chronicles', '(', 'japanese', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media', '.', 'vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', 'nameless', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '<unk>', 'raven', '.']\n",
      "Formatted Text: [0, 83, 3849, 88, 0, 3869, 21, 780, 0, 2, 0, 3, 3849, 4, 1, 0, 88, 20, 2, 1837, 1018, 7, 14, 3849, 3869, 881, 629, 976, 2, 23, 8, 0, 299, 12, 575, 232, 67, 452, 19, 0, 5, 757, 3, 2500, 17, 1, 1767, 0, 3, 155, 6, 246, 354, 6, 976, 2, 24, 23, 1, 237, 67, 6, 1, 3849, 93, 3, 0, 1, 156, 0, 4, 0, 5, 729, 12, 58, 2096, 14, 43, 0, 2, 1, 333, 1085, 3218, 7, 1, 37, 67, 5, 1694, 1, 0, 2, 8, 0, 313, 1063, 2082, 1, 1702, 4, 0, 56, 1, 95, 0, 107, 52, 1938, 1644, 288, 598, 5, 34, 0, 120, 1, 2321, 1063, 0, 0, 3]\n",
      "Length of tokens: 123\n",
      "-----------------\n",
      "Current data:  The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      " Length: 520\n",
      " Tokenized: ['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', ',', 'it', 'also', 'underwent', 'multiple', 'adjustments', ',', 'such', 'as', 'making', 'the', 'game', 'more', '<unk>', 'for', 'series', 'newcomers', '.', 'character', 'designer', '<unk>', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', ',', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', '.', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', 'the', 'game', \"'\", 's', 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'\", 'n', '.']\n",
      "Formatted Text: [1, 67, 135, 369, 6, 297, 2, 3245, 65, 8, 184, 1742, 4, 1, 138, 1177, 13, 3849, 3869, 304, 3, 66, 24, 3277, 1, 1176, 579, 4, 1, 93, 2, 24, 44, 0, 1842, 0, 2, 89, 14, 407, 1, 67, 61, 0, 17, 93, 0, 3, 278, 3749, 0, 0, 5, 3024, 0, 0, 99, 435, 25, 479, 0, 2, 163, 18, 3849, 3869, 304, 537, 0, 0, 3, 8, 184, 157, 4, 1145, 3886, 1, 1623, 3, 1, 67, 11, 15, 658, 1071, 10, 3610, 19, 75, 11, 1586, 3]\n",
      "Length of tokens: 93\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# Run down of the pipeline\n",
    "for data in data_iter[:5]:\n",
    "    print(f\"Current data: {data}\")\n",
    "    print(f\" Length: {len(data)}\")\n",
    "    print(f\" Tokenized: {tokenizer(data)}\")\n",
    "    print(f\"Formatted Text: {text_pipeline(data)}\")\n",
    "    print(f\"Length of tokens: {len(text_pipeline(data))}\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: [281, 144, 88, 181, 245, 172, 30, 48, 89, 231, 298, 365, 17, 320]\n",
      "SNW tokens: [281, 144, 88, 181, 245, 172, 30, 48, 89]\n",
      "Input Tokens: 245\n",
      "Output Tokens: [281, 144, 88, 181, 172, 30, 48, 89]\n",
      "Batched Input\n",
      "[245, 245, 245, 245, 245, 245, 245, 245]\n",
      "Batched Output\n",
      "[281, 144, 88, 181, 172, 30, 48, 89]\n"
     ]
    }
   ],
   "source": [
    "# Run down of th collate function\n",
    "import numpy as np\n",
    "SNW = 4\n",
    "text_tokens_ids = list(np.random.randint(0, 400, (14)))\n",
    "batch_input, batch_output = [], []\n",
    "print(f\"Input Tokens: {text_tokens_ids}\")\n",
    "for idx in range(len(text_tokens_ids) - SNW * 2):\n",
    "    token_id_sequence = text_tokens_ids[idx : (idx + SNW * 2 + 1)]\n",
    "    print(f\"SNW tokens: {token_id_sequence}\")\n",
    "    input_ = token_id_sequence.pop(SNW)\n",
    "    outputs = token_id_sequence\n",
    "    print(f\"Input Tokens: {input_}\")\n",
    "    print(f\"Output Tokens: {outputs}\")\n",
    "\n",
    "    for output in outputs: \n",
    "        batch_input.append(input_)\n",
    "        batch_output.append(output)\n",
    "\n",
    "    print(\"Batched Input\")\n",
    "    print(batch_input)\n",
    "    print(\"Batched Output\")\n",
    "    print(batch_output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the inputs and outputs for training\n",
    "def collate_skipgram(batch, text_pipeline):\n",
    "    batch_input, batch_output = [], []\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
    "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
    "            outputs = token_id_sequence\n",
    "\n",
    "            for output in outputs:\n",
    "                batch_input.append(input_)\n",
    "                batch_output.append(output)\n",
    "    \n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data loading part\n",
    "dataloader = DataLoader(data_iter, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(collate_skipgram, text_pipeline = text_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram Model\n",
    "class SkipGram_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_DIMENSION\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "model = SkipGram_Model(len(vocab))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 10 Loss: 7.26474142074585\n",
      "Training Step: 20 Loss: 6.620956897735596\n",
      "Training Step: 30 Loss: 6.205772399902344\n",
      "Training Step: 40 Loss: 6.165273189544678\n",
      "Training Step: 50 Loss: 6.0563435554504395\n",
      "Training Step: 60 Loss: 5.704689025878906\n",
      "Training Step: 70 Loss: 5.651385307312012\n",
      "Training Step: 80 Loss: 5.83539342880249\n",
      "Training Step: 90 Loss: 5.865797996520996\n",
      "Training Step: 100 Loss: 5.726323127746582\n",
      "Training Step: 110 Loss: 5.774597644805908\n",
      "Training Step: 120 Loss: 5.651203632354736\n",
      "Training Step: 130 Loss: 5.747607707977295\n",
      "Training Step: 140 Loss: 5.60974645614624\n",
      "Training Step: 150 Loss: 5.746978759765625\n",
      "Training Step: 160 Loss: 5.562025547027588\n",
      "Training Step: 170 Loss: 5.397178649902344\n",
      "Training Step: 180 Loss: 5.892734050750732\n",
      "Training Step: 190 Loss: 5.66327428817749\n",
      "Training Step: 200 Loss: 5.532326698303223\n",
      "Training Step: 210 Loss: 5.579980373382568\n",
      "Training Step: 220 Loss: 5.450323581695557\n",
      "Training Step: 230 Loss: 5.710562229156494\n",
      "Training Step: 240 Loss: 5.7064361572265625\n",
      "Training Step: 250 Loss: 5.365702152252197\n",
      "Training Step: 260 Loss: 5.604966163635254\n",
      "Training Step: 270 Loss: 5.754703998565674\n",
      "Training Step: 280 Loss: 5.504042148590088\n",
      "Training Step: 290 Loss: 5.589004993438721\n",
      "Training Step: 300 Loss: 5.553156852722168\n",
      "Training Step: 310 Loss: 5.5678300857543945\n",
      "Training Step: 320 Loss: 5.5687432289123535\n",
      "Training Step: 330 Loss: 5.595533847808838\n",
      "Training Step: 340 Loss: 5.463494300842285\n",
      "Training Step: 350 Loss: 5.53878927230835\n",
      "Training Step: 360 Loss: 5.736812114715576\n",
      "Training Step: 370 Loss: 5.530311584472656\n",
      "Training Step: 380 Loss: 5.548991680145264\n",
      "Training Step: 390 Loss: 5.645244121551514\n",
      "Training Step: 400 Loss: 5.752128601074219\n",
      "Training Step: 410 Loss: 5.505754470825195\n",
      "Training Step: 420 Loss: 5.572589874267578\n",
      "Training Step: 430 Loss: 5.469390869140625\n",
      "Training Step: 440 Loss: 5.44991397857666\n",
      "Training Step: 450 Loss: 5.657690525054932\n",
      "Training Step: 460 Loss: 5.503820419311523\n",
      "Training Step: 470 Loss: 5.712265968322754\n",
      "Training Step: 480 Loss: 5.568580150604248\n",
      "Training Step: 490 Loss: 5.395323276519775\n",
      "Training Step: 500 Loss: 5.671262264251709\n",
      "Training Step: 510 Loss: 5.628809928894043\n",
      "Training Step: 520 Loss: 5.561861038208008\n",
      "Training Step: 530 Loss: 5.633459568023682\n",
      "Training Step: 540 Loss: 5.5408196449279785\n",
      "Training Step: 550 Loss: 5.488986492156982\n",
      "Training Step: 560 Loss: 5.638077735900879\n",
      "Training Step: 570 Loss: 5.6125688552856445\n",
      "Training Step: 580 Loss: 5.712876796722412\n",
      "Training Step: 590 Loss: 5.488534450531006\n",
      "Training Step: 600 Loss: 5.483560562133789\n",
      "Training Step: 610 Loss: 5.271910667419434\n",
      "Training Step: 620 Loss: 5.535338401794434\n",
      "Training Step: 630 Loss: 5.520744323730469\n",
      "Training Step: 640 Loss: 5.568958759307861\n",
      "Training Step: 650 Loss: 5.781559944152832\n",
      "Training Step: 660 Loss: 5.604284286499023\n",
      "Training Step: 670 Loss: 5.326485633850098\n",
      "Training Step: 680 Loss: 5.634907245635986\n",
      "Training Step: 690 Loss: 5.522701740264893\n",
      "Training Step: 700 Loss: 5.512252330780029\n",
      "Training Step: 710 Loss: 5.392745494842529\n",
      "Training Step: 720 Loss: 5.427556037902832\n",
      "Training Step: 730 Loss: 5.4152655601501465\n",
      "Training Step: 740 Loss: 5.619812965393066\n",
      "Training Step: 750 Loss: 5.453371524810791\n",
      "Training Step: 760 Loss: 5.416074752807617\n",
      "Training Step: 770 Loss: 5.663108825683594\n",
      "Training Step: 780 Loss: 5.583926200866699\n",
      "Training Step: 790 Loss: 5.4426069259643555\n",
      "Training Step: 800 Loss: 5.546346664428711\n",
      "Training Step: 810 Loss: 5.549857139587402\n",
      "Training Step: 820 Loss: 5.522313117980957\n",
      "Training Step: 830 Loss: 5.7185163497924805\n",
      "Training Step: 840 Loss: 5.571249008178711\n",
      "Training Step: 850 Loss: 5.61981725692749\n",
      "Training Step: 860 Loss: 5.573354721069336\n",
      "Training Step: 870 Loss: 5.567341327667236\n",
      "Training Step: 880 Loss: 5.580654144287109\n",
      "Training Step: 890 Loss: 5.613637924194336\n",
      "Training Step: 900 Loss: 5.486505508422852\n",
      "Training Step: 910 Loss: 5.438399791717529\n",
      "Training Step: 920 Loss: 5.576967239379883\n",
      "Training Step: 930 Loss: 5.435192584991455\n",
      "Training Step: 940 Loss: 5.495716094970703\n",
      "Training Step: 950 Loss: 5.801992893218994\n",
      "Training Step: 960 Loss: 5.517748832702637\n",
      "Training Step: 970 Loss: 5.533019065856934\n",
      "Training Step: 980 Loss: 5.522600173950195\n",
      "Training Step: 990 Loss: 5.627349853515625\n",
      "Training Step: 1000 Loss: 5.551711082458496\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "running_loss = []\n",
    "for i, batch_data in enumerate(dataloader, 1):\n",
    "    \n",
    "    inputs = batch_data[0]\n",
    "    labels = batch_data[1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = loss_fn(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss.append(loss.item())\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Training Step: {i} Loss: {loss.item()}\")\n",
    "    \n",
    "    if i == 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference NOTE: The model was not trained much. More training leads to better performance\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "tokens = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get similar words\n",
    "def get_top_similar(word: str, topN: int = 10):\n",
    "    word_id = vocab[word]\n",
    "    if word_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "\n",
    "    word_vec = embeddings[word_id]\n",
    "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
    "    dists = np.matmul(embeddings, word_vec).flatten()\n",
    "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
    "\n",
    "    topN_dict = {}\n",
    "    for sim_word_id in topN_ids:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        dist = dists[sim_word_id]\n",
    "        topN_dict[sim_word] = np.round(dist, 3)\n",
    "    return topN_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zealand': 37.545,\n",
       " 'vietnam': 35.851,\n",
       " '130': 35.637,\n",
       " 'peach': 35.626,\n",
       " 'onto': 32.696,\n",
       " 'bay': 32.297,\n",
       " 'siege': 32.133,\n",
       " '1940': 32.068,\n",
       " 'focus': 32.03,\n",
       " 'operating': 31.58}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_similar(\"uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 103.393\n",
      "woman: 100.410\n",
      "egypt: 53.854\n",
      "contributed: 51.014\n",
      "2015: 49.980\n"
     ]
    }
   ],
   "source": [
    "emb1 = embeddings[vocab[\"king\"]]\n",
    "emb2 = embeddings[vocab[\"man\"]]\n",
    "emb3 = embeddings[vocab[\"woman\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
